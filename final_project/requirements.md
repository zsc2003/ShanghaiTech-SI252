# Topics for Final Project

**1.  Theory & Algorithm Design**

**1.1 Offline RL with Diffusion Model**

Levine, Sergey, et al. "Offline reinforcement learning: Tutorial, review, and perspectives on open problems." *arXiv preprint arXiv:2005.01643* (2020).

Chen, Minshuo, et al. "An overview of diffusion models: Applications, guided generation, statistical rates and optimization." *arXiv preprint arXiv:2404.07771* (2024).

Efficient Diffusion Model: <https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey> 

Jackson, Matthew Thomas, et al. "A Clean Slate for Offline Reinforcement Learning." *arXiv preprint arXiv:2504.11453* (2025). 

Prudencio, Rafael Figueiredo, Marcos ROA Maximo, and Esther Luna Colombini. "A survey on offline reinforcement learning: Taxonomy, review, and open problems." *IEEE Transactions on Neural Networks and Learning Systems* (2023).

Janner, Michael, Qiyang Li, and Sergey Levine. "Offline reinforcement learning as one big sequence modeling problem." *Advances in neural information processing systems* 34 (2021): 1273-1286.

Chen, Lili, et al. "Decision transformer: Reinforcement learning via sequence modeling." *Advances in neural information processing systems* 34 (2021): 15084-15097.

Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. "Diffusion policies as an expressive policy class for offline reinforcement learning." *arXiv preprint arXiv:2208.06193* (2022).

Kang, Bingyi, et al. "Efficient diffusion policies for offline reinforcement learning." *Advances in Neural Information Processing Systems* 36 (2023): 67195-67212. 

Qiao, RuiXi, et al. "Offline Reinforcement Learning with Discrete Diffusion Skills." *arXiv preprint arXiv:2503.20176* (2025). 

Ma, Haitong, et al. "Soft Diffusion Actor-Critic: Efficient Online Reinforcement Learning for Diffusion Policy." *arXiv preprint arXiv:2502.00361* (2025). 

Duan, Xintong, et al. "State Combinatorial Generalization In Decision Making With Conditional Diffusion Models." *arXiv preprint arXiv:2501.13241* (2025).

**1.2  Offline Bandit Learning**

Sentenac, Flore, Ilbin Lee, and Csaba Szepesvari. "Balancing optimism and pessimism in offline-to-online learning." *arXiv preprint arXiv:2502.08259* (2025)

Rashidinejad, Paria, et al. "Bridging offline reinforcement learning and imitation learning: A tale of pessimism." *Advances in Neural Information Processing Systems* 34 (2021): 11702-11716.

Agnihotri, Akhil, et al. "Online bandit learning with offline preference data." *arXiv preprint arXiv:2406.09574* (2024). 

Liu, Xutong, et al. "Offline Learning for Combinatorial Multi-armed Bandits." *arXiv preprint arXiv:2501.19300* (2025). 

Li, Ye, et al. "Unifying offline causal inference and online bandit learning for data driven decision." *Proceedings of the Web Conference 2021*. 2021. 

Sakhi, Otmane, Pierre Alquier, and Nicolas Chopin. "PAC-Bayesian offline contextual bandits with guarantees." *International Conference on Machine Learning*. PMLR, 2023.

Sakhi, Otmane. *Offline Contextual Bandit: Theory and Large Scale Applications*. Diss. Institut Polytechnique de Paris, 2023.  

Yang, Zhouhao, et al. "Distributionally robust policy gradient for offline contextual bandits." *International Conference on Artificial Intelligence and Statistics*. PMLR, 2023. 

Nguyen-Tang, Thanh, et al. "Offline neural contextual bandits: Pessimism, optimization and generalization." *arXiv preprint arXiv:2111.13807* (2021).

Zhang, Kelly W., et al. "Contextual Thompson Sampling via Generation of Missing Data." *arXiv preprint arXiv:2502.07064* (2025).

Lyu, Z., Li, Y., Zhu, G., Xu, J., Poor, H. V., & Cui, S. (2024). Rethinking resource management in edge learning: A joint pre-training and fine-tuning design paradigm. *IEEE Transactions on Wireless Communications*. 

**1.3  Multi-Agent Bandit Learning**

Boursier, Etienne, and Vianney Perchet. "A survey on multi-player bandits." *Journal of Machine Learning Research* 25.137 (2024): 1-45.

Wang, Xuchuang, et al. "Asynchronous Multi-Agent Bandits: Fully Distributed vs. Leader-Coordinated Algorithms." *Proceedings of the ACM on Measurement and Analysis of Computing Systems* 9.1 (2025): 1-39.

Mirfakhar, Amirmahdi, et al. "Heterogeneous Multi-Agent Bandits with Parsimonious Hints." *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 39. No. 18. 2025.

Li, Hongbo, and Lingjie Duan. "Competitive Multi-armed Bandit Games for Resource Sharing." *IEEE Transactions on Mobile Computing* (2025). 

Fourati, Fares, Mohamed-Slim Alouini, and Vaneet Aggarwal. "Federated combinatorial multi-agent multi-armed bandits." *arXiv preprint arXiv:2405.05950* (2024).

Shao, Junning, Siwei Wang, and Zhixuan Fang. "Learning with Limited Shared Information in Multi-agent Multi-armed Bandit." *arXiv preprint arXiv:2502.15338* (2025).

Zhu, Jingxuan, et al. "Decentralized Upper Confidence Bound Algorithms for Homogeneous Multi-Agent Multi-Armed Bandits." *IEEE Transactions on Automatic Control* (2025).

**1.4  Multi-Agent Reinforcement Learning** 

Zhang, Kaiqing, Zhuoran Yang, and Tamer Başar. "Multi-agent reinforcement learning: A selective overview of theories and algorithms." *Handbook of reinforcement learning and control* (2021): 321-384. 

Nguyen, Thanh Thi, Ngoc Duy Nguyen, and Saeid Nahavandi. "Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications." *IEEE transactions on cybernetics* 50.9 (2020): 3826-3839.

Wong, Annie, et al. "Deep multiagent reinforcement learning: Challenges and directions." *Artificial Intelligence Review* 56.6 (2023): 5023-5056.

Gronauer, Sven, and Klaus Diepold. "Multi-agent deep reinforcement learning: a survey." *Artificial Intelligence Review* 55.2 (2022): 895-943.

Hernandez-Leal, Pablo, Bilal Kartal, and Matthew E. Taylor. "A survey and critique of multiagent deep reinforcement learning." *Autonomous Agents and Multi-Agent Systems* 33.6 (2019): 750-797.

Rutherford, Alexander, et al. "Jaxmarl: Multi-agent rl environments and algorithms in jax." *Advances in Neural Information Processing Systems* 37 (2024): 50925-50951. 

Jin, Chi, et al. "V-learning—a simple, efficient, decentralized algorithm for multiagent reinforcement learning." *Mathematics of Operations Research* 49.4 (2024): 2295-2322. 

Sun, Chuanneng, Songjun Huang, and Dario Pompili. "Llm-based multi-agent reinforcement learning: Current and future directions." *arXiv preprint arXiv:2405.11106* (2024). 

Bettini, Matteo, Amanda Prorok, and Vincent Moens. "Benchmarl: Benchmarking multi-agent reinforcement learning." *Journal of Machine Learning Research* 25.217 (2024): 1-10. 

Zhang, Ruiqi, et al. "Multi-agent reinforcement learning for autonomous driving: A survey." *arXiv preprint arXiv:2408.09675* (2024). 

**2. Application**

**2.1  Federated/Distributed CG&CV with RL/Bandit Learning**

Holden, L., Dayoub, F., Harvey, D., & Chin, T. J. (2023). Federated neural radiance fields. *arXiv preprint arXiv:2305.01163*. 

Suzuki, T. (2023). Federated learning for large-scale scene modeling with neural radiance fields. *arXiv preprint arXiv:2309.06030*. 

Suzuki, T. (2024). Fed3dgs: Scalable 3d gaussian splatting with federated learning. *arXiv preprint arXiv:2403.11460*.  

Tasneem, Zaid, et al. "DecentNeRFs: Decentralized Neural Radiance Fields from Crowdsourced Images." *European Conference on Computer Vision*. Cham: Springer Nature Switzerland, 2024.

Guo, Y., Qin, Z., Tao, X., & Li, G. Y. (2023). Federated multi-view synthesizing for metaverse. *IEEE Journal on Selected Areas in Communications*, *42*(4), 867-879. 

Tharakan, K. S., Dahrouj, H., Kouzayha, N., Elsawy, H., & Al-Naffouri, T. Y. (2025). Personalized Federated Learning for Cellular VR: Online Learning and Dynamic Caching. *IEEE Transactions on Communications*.  

Wu, G., Lyu, Z., Zhang, J., & Xu, J. (2024). Embracing radiance field rendering in 6G: Over-the-air training and inference with 3D contents. *IEEE Open Journal of the Communications Society*. 

**2.2    NLP with Bandit Learning /RL**

Bouneffouf, D., & Féraud, R. (2024, August). A tutorial on multi-armed bandit applications for large language models. In *Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining* (pp. 6412-6413).

Moerchen, F., Ernst, P., & Zappella, G. (2020, October). Personalizing natural language understanding using multi-armed bandits and implicit feedback. In *Proceedings of the 29th ACM international conference on information & knowledge management* (pp. 2661-2668).

S. Wang, Z. Shao, and J. Lui, ‘‘Next-Word Prediction: A Perspective of Energy-Aware Distributed Inference’’, IEEE Transactions on Mobile Computing, vol. 23, no. 5, pp. 5695 - 5708, May 2024.

Urteaga, Iñigo, et al. "Multi-armed bandits for resource efficient, online optimization of language model pre-training: the use case of dynamic masking." *arXiv preprint arXiv:2203.13151* (2022). 

Uc-Cetina, Victor, et al. "Survey on reinforcement learning for language processing." *Artificial Intelligence Review* 56.2 (2023): 1543-1575.

Wang, William Yang, Jiwei Li, and Xiaodong He. "Deep reinforcement learning for NLP." *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts*. 2018. 

Li, Shuang, et al. "Pre-trained language models for interactive decision-making." *Advances in Neural Information Processing Systems* 35 (2022): 31199-31212.

Carta, Thomas, et al. "Grounding large language models in interactive environments with online reinforcement learning." *International Conference on Machine Learning*. PMLR, 2023.

Du, Yuqing, et al. "Guiding pretraining in reinforcement learning with large language models." *International Conference on Machine Learning*. PMLR, 2023.

**2.3  LLM with Reinforcement Learning** 

Sun, Chuanneng, Songjun Huang, and Dario Pompili. "Llm-based multi-agent reinforcement learning: Current and future directions." *arXiv preprint arXiv:2405.11106* (2024). 

Hu, Bin, et al. "Enabling intelligent interactions between an agent and an LLM: A reinforcement learning approach." *arXiv preprint arXiv:2306.03604* (2023). 

Yu, Qiying, et al. "Dapo: An open-source llm reinforcement learning system at scale." *arXiv preprint arXiv:2503.14476* (2025).

Wang, Boyuan, et al. "LLM-empowered state representation for reinforcement learning." *arXiv preprint arXiv:2407.13237* (2024).

Xie, Tian, et al. "Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning." *arXiv preprint arXiv:2502.14768* (2025).

Peiyuan, Feng, et al. "AGILE: A Novel Reinforcement Learning Framework of LLM Agents." *Advances in Neural Information Processing Systems* 37 (2024): 5244-5284.

Chen, Kevin, et al. "Reinforcement Learning for Long-Horizon Interactive LLM Agents." *arXiv preprint arXiv:2502.01600* (2025).

Wang, Huaijie, et al. "Offline Reinforcement Learning for LLM Multi-Step Reasoning." *arXiv preprint arXiv:2412.16145* (2024). 

Silvestre, Pedro F., and Peter Pietzuch. "Systems Opportunities for LLM Fine-Tuning using Reinforcement Learning." *Proceedings of the 5th Workshop on Machine Learning and Systems*. 2025.  

Cao, Yuji, et al. "Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods." *IEEE Transactions on Neural Networks and Learning Systems* (2024). 

Chakraborty, Souradip, et al. "Transfer q-star: Principled decoding for LLM alignment." *Advances in Neural Information Processing Systems* 37 (2024): 101725-101761. 

Ding, Mucong, et al. "Sail: Self-improving efficient online alignment of large language models." *arXiv preprint arXiv:2406.15567* (2024). 

**2.4  Robotics with Reinforcement Learning** 

Tang, Chen, et al. "Deep reinforcement learning for robotics: A survey of real-world successes." *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 39. No. 27. 2025. 

Singh, Bharat, Rajesh Kumar, and Vinay Pratap Singh. "Reinforcement learning in robotic applications: a comprehensive survey." *Artificial Intelligence Review* 55.2 (2022): 945-990. 

Gu, Shixiang, et al. "Deep reinforcement learning for robotic manipulation." *arXiv preprint arXiv:1610.00633* 1.1 (2016).

Zhu, Henry, et al. "The ingredients of real-world robotic reinforcement learning." *arXiv preprint arXiv:2004.12570* (2020). 

Kalashnikov, Dmitry, et al. "Scalable deep reinforcement learning for vision-based robotic manipulation." *Conference on robot learning*. PMLR, 2018. 

Singh, Avi, et al. "End-to-end robotic reinforcement learning without reward engineering." *arXiv preprint arXiv:1904.07854* (2019). 

Ibarz, Julian, et al. "How to train your robot with deep reinforcement learning: lessons we have learned." *The International Journal of Robotics Research* 40.4-5 (2021): 698-721.

**2.5  EDA & Circuit Design with  Reinforcement Learning**

Huang, Guyue, et al. "Machine learning for electronic design automation: A survey." *ACM Transactions on Design Automation of Electronic Systems (TODAES)* 26.5 (2021): 1-46.

Budak, Ahmet F., et al. "Reinforcement learning for electronic design automation: Case studies and perspectives." *2022 27th Asia and South Pacific Design Automation Conference (ASP-DAC)*. IEEE, 2022.

Zhu, Huming, and Xiangru Chen. "A survey of Reinforcement Learning for Electronic Design Automation." *2024 2nd International Symposium of Electronics Design Automation (ISEDA)*. IEEE, 2024.

Ren, Haoxing, and Jiang Hu, eds. *Machine learning applications in electronic design automation*. Berlin: Springer, 2022.

Liu, Kang, et al. "Can we trust machine learning for electronic design automation?." *2021 IEEE 34th International System-on-Chip Conference (SOCC)*. IEEE, 2021. 

Bao, Jiarui, et al. "Multiagent Based Reinforcement Learning (MA-RL): An Automated Designer for Complex Analog Circuits." *IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems* (2024).

**2.6  Protein Design with Reinforcement Learning**

Lutz, Isaac D., et al. "Top-down design of protein architectures with reinforcement learning." *Science* 380.6642 (2023): 266-273. 

Wang, Yi, et al. "Self-play reinforcement learning guides protein engineering." *Nature Machine Intelligence* 5.8 (2023): 845-860. 

Subramanian, Jithendaraa, et al. "Reinforcement Learning for Sequence Design Leveraging Protein Language Models." *arXiv preprint arXiv:2407.03154* (2024). 

Zhang, Yunjiang, et al. "Universal approach to de novo drug design for target proteins using deep reinforcement learning." *ACS omega* 8.6 (2023): 5464-5474. 

McNaughton, Andrew D., et al. "De novo design of protein target specific scaffold-based Inhibitors via Reinforcement Learning." *arXiv preprint arXiv:2205.10473* (2022). 

Angermueller, Christof, et al. "Model-based reinforcement learning for biological sequence design." *International conference on learning representations*. 2019. 

Sternke, Matt, and Joel Karpiak. "ProteinRL: Reinforcement learning with generative protein language models for property-directed sequence design." *NeurIPS 2023 Generative AI and Biology (GenBio) Workshop*. 2023. 

Renard, Frederic, et al. "Model-based reinforcement learning for protein backbone design." *arXiv preprint arXiv:2405.01983* (2024). 

Gao, Ziqi, et al. "Deep Reinforcement Learning for Modelling Protein Complexes." *arXiv preprint arXiv:2405.02299* (2024). 





# Time Line of Final Project

Timeline: 

Send your project proposal to TA and Professor before 11:59pm, May 04 (the end of the eleventh week). 

Confirmed by Professor, then build a GitHub page for your project and send the link  to TA and Professor before 11:59pm May 09

Provide weekly progress reports in your Github page

Prepare and present the poster on June 06(the last day of the course)





# One Method of Paper Reading

To understand state-of-the-art results, it is important to read and parse research papers. If this is your first time reading a research paper,  this guide may be helpful.

[How to Read Paper.pdf](https://cdn-uploads.piazza.com/paste/i911kz7rgjc7bx/d621df366fc99c6ce0e096817b6f14a084821352f173d6ef02b31eb237932197/HowtoReadPaper.pdf)

This resource may be even useful for veterans in research. It describes a multi-pass approach to reading papers. The papers that you can choose to read may vary in difficulty, and we do not expect anyone to understand all of the content in the papers. Nonetheless, it is important to start building your ability to read research papers, especially research papers out of your comfort zone. The notation, mathematics, and jargon of the paper matter less than the big ideas and context of the paper. Read papers, take notes, and answer the questions for each paper individually (see below).

Questions for each paper：

- What are the papers’ main contributions? Describe the core idea.
- What was surprising, difficult, or confusing in the paper?
- Were the experiments (if there are any) convincing?
- How can these methods be applied in ways not described in the paper? Have you seen these ideas in other papers? Feel free to cite relevant work.